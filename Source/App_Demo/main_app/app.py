# File: main_app/app.py
import gradio as gr
import torch
import requests
import os
from PIL import Image
from transformers import CLIPProcessor, ViltProcessor
# Import c·∫£ CLIP v√† h√†m l·∫•y ViLT t·ª´ file model_class
from model_class import MultimodalCLIPClassifier, get_vilt_model

# --- C·∫§U H√åNH ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLIP_PATH = "../checkpoints/clip_qa_cap_slo_ocr.pth"
VILT_PATH = "../checkpoints/vilt_qa_cap_slo_ocr.pth" # ƒê∆∞·ªùng d·∫´n file ViLT

OCR_API_URL = "http://127.0.0.1:8000/ocr"
GEN_API_URL = "http://127.0.0.1:8001/generate"

# Mapping ID sang T√™n (D·ª±a tr√™n list b·∫°n cung c·∫•p)
TOPIC_NAMES = {
    1: "Restaurants, cafe, fast food",
    2: "Chocolate, cookies, candy, ice cream",
    3: "Chips, snacks, nuts, fruit...",
    4: "Seasoning, condiments, ketchup",
    5: "Alcohol",
    6: "Coffee, tea",
    7: "Soda, juice, milk, energy drinks",
    8: "Cars, automobiles",
    9: "Electronics",
    10: "Phone, TV and internet service",
    11: "Financial services",
    12: "Other services",
    13: "Beauty products and cosmetics",
    14: "Healthcare and medications",
    15: "Clothing and accessories",
    16: "Games and toys",
    17: "Home appliances",
    18: "Vacation and travel",
    19: "Media and arts",
    20: "Sports equipment and activities",
    21: "Shopping and retail products",
    22: "Environment, nature, pollution",
    23: "Animals & Pet Care",
    24: "Safety, security and social awareness",
    25: "Smoking, alcohol abuse",
    26: "Unclear or mixed content"
}

# --- 1. LOAD MODELS ---
print(f"Using Device: {DEVICE}")

# A. Load CLIP
print("‚è≥ Loading CLIP Model...")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32", use_fast=True)
clip_model = MultimodalCLIPClassifier(num_classes=26).to(DEVICE)
try:
    clip_model.load_state_dict(torch.load(CLIP_PATH, map_location=DEVICE, weights_only=True), strict=False)
    clip_model.eval()
    print("‚úÖ CLIP Ready!")
except Exception as e:
    print(f"‚ùå CLIP Load Error: {e}")

# B. Load ViLT
print("‚è≥ Loading ViLT Model...")
vilt_processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm", use_fast=True)
vilt_model = get_vilt_model(num_classes=26, device=DEVICE)
try:
    # Load weights
    vilt_model.load_state_dict(torch.load(VILT_PATH, map_location=DEVICE, weights_only=True), strict=False)
    vilt_model.eval()
    print("‚úÖ ViLT Ready!")
except Exception as e:
    print(f"‚ùå ViLT Load Error: {e}")

# --- 2. HELPER FUNCTIONS (API Calls) ---
# (Gi·ªØ nguy√™n nh∆∞ c≈©)
def call_ocr_service(image_path):
    try:
        with open(image_path, 'rb') as f:
            response = requests.post(OCR_API_URL, files={'file': f}, timeout=30)
        if response.status_code == 200 and response.json().get("status") == "success":
            return response.json()["text"]
        return ""
    except: return ""

def call_gen_service(image_path, task):
    try:
        with open(image_path, 'rb') as f:
            data = {"task": task}
            response = requests.post(GEN_API_URL, files={'file': f}, data=data, timeout=60)
        if response.status_code == 200:
            return response.json().get("result", "")
        return ""
    except: return ""

# --- 3. PREDICT LOGIC (C·∫≠p nh·∫≠t x·ª≠ l√Ω ch·∫ø ƒë·ªô Image Only) ---
def analyze_and_predict(image, user_ocr, user_caption, user_slogan, selected_model, enable_auto_gen):
    if image is None: return "Upload image first!", "", "", "", {}
    
    # Save Temp Image
    os.makedirs("../temp", exist_ok=True)
    temp_path = "../temp/query.jpg"
    image.save(temp_path)
    
    # Ki·ªÉm tra xem c√°c √¥ c√≥ tr·ªëng kh√¥ng
    all_empty = (not user_ocr.strip()) and (not user_caption.strip()) and (not user_slogan.strip())

    # --- LOGIC QUAN TR·ªåNG ---
    # Ch·ªâ Auto-fill khi: (T·∫•t c·∫£ ƒë·ªÅu tr·ªëng) V√Ä (Checkbox Auto-gen ƒëang B·∫¨T)
    if all_empty and enable_auto_gen:
        final_ocr = call_ocr_service(temp_path)
        final_caption = call_gen_service(temp_path, "caption")
        final_slogan = call_gen_service(temp_path, "slogan")
    else:
        # N·∫øu Checkbox t·∫Øt (Ch·∫ø ƒë·ªô Image Only) -> Gi·ªØ nguy√™n text r·ªóng
        final_ocr = user_ocr
        final_caption = user_caption
        final_slogan = user_slogan

    # N·∫øu ch·∫°y ch·∫ø ƒë·ªô Image Only, full_text s·∫Ω r·ªóng -> G√°n label m·∫∑c ƒë·ªãnh ƒë·ªÉ tr√°nh l·ªói model
    full_text = f"{final_caption} {final_slogan} {final_ocr}".strip()
    if not full_text: full_text = "image" 
    
    probs = None
    
    # --- Step 2: Inference ---
    if selected_model == "CLIP (Original)":
        inputs = clip_processor(
            text=[full_text], images=image, return_tensors="pt", 
            padding="max_length", truncation=True, max_length=77
        )
        inputs["input_ids"] = inputs["input_ids"].unsqueeze(1)
        inputs["attention_mask"] = inputs["attention_mask"].unsqueeze(1)
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
        
        with torch.no_grad():
            logits = clip_model(inputs["pixel_values"], inputs["input_ids"], inputs["attention_mask"])
            probs = torch.softmax(logits, dim=1)[0]

    else: # ViLT
        inputs = vilt_processor(
            images=image, text=full_text, return_tensors="pt",
            padding="max_length", truncation=True, max_length=40
        )
        
        # --- FIX L·ªñI DIMENSION ---
        # Lo·∫°i b·ªè pixel_mask v√¨ n√≥ g√¢y l·ªói dimension (3D vs 4D conflict)
        if "pixel_mask" in inputs:
            inputs.pop("pixel_mask")
            
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()} # ƒê·∫©y data sang GPU/CPU sau khi pop
        
        with torch.no_grad():
            outputs = vilt_model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)[0]

    # --- Step 3: Result ---
    top5_prob, top5_idx = torch.topk(probs, 5)
    results = {}
    for i in range(5):
        topic_name = TOPIC_NAMES.get(top5_idx[i].item() + 1, "Unknown")
        results[topic_name] = top5_prob[i].item()
        
    return full_text, final_ocr, final_caption, final_slogan, results

# --- 4. INTERFACE ---
with gr.Blocks(title="Ads Classifier Ultimate") as demo:
    gr.Markdown("# üöÄ Ads Classifier: CLIP vs ViLT")
    
    with gr.Row():
        with gr.Column(scale=1):
            input_img = gr.Image(type="pil", label="·∫¢nh Qu·∫£ng C√°o")
            
            model_selector = gr.Radio(
                ["CLIP (Original)", "ViLT (New)"], 
                label="Ch·ªçn Model D·ª± ƒêo√°n", 
                value="CLIP (Original)"
            )

            # --- CHECKBOX CH·∫æ ƒê·ªò IMAGE ONLY ---
            # D√πng Checkbox ƒë·ªÉ c√≥ tr·∫°ng th√°i True/False r√µ r√†ng
            chk_image_only = gr.Checkbox(
                label="üëÅÔ∏è Ch·∫ø ƒë·ªô ch·ªâ d√πng ·∫¢nh (Image Only)", 
                value=False, # M·∫∑c ƒë·ªãnh kh√¥ng t√≠ch
                info="T√≠ch v√†o ƒë·ªÉ ·∫©n Text v√† t·∫Øt t√≠nh nƒÉng t·ª± ƒë·ªông t·∫°o n·ªôi dung."
            )
            
            # --- Accordion Text ---
            # G√°n v√†o bi·∫øn acc_text_group ƒë·ªÉ code c√≥ th·ªÉ ƒëi·ªÅu khi·ªÉn ƒë√≥ng/m·ªü
            with gr.Accordion("Chi ti·∫øt Text (AI Generated)", open=True) as acc_text_group:
                # Checkbox con (·∫©n ho·∫∑c ƒë·ªìng b·ªô logic)
                chk_auto_gen = gr.Checkbox(label="T·ª± ƒë·ªông t·∫°o Text n·∫øu thi·∫øu", value=True, visible=False) 
                
                with gr.Row():
                    txt_slogan = gr.Textbox(label="Slogan", placeholder="...", lines=2, max_lines=5, scale=8)
                    btn_cls_slogan = gr.Button("‚ùå", scale=1, min_width=10)
                with gr.Row():
                    txt_caption = gr.Textbox(label="Caption", placeholder="...", lines=3, max_lines=10, scale=8)
                    btn_cls_caption = gr.Button("‚ùå", scale=1, min_width=10)
                with gr.Row():
                    txt_ocr = gr.Textbox(label="OCR", placeholder="...", lines=4, max_lines=20, scale=8)
                    btn_cls_ocr = gr.Button("‚ùå", scale=1, min_width=10)
                
                btn_clear_all = gr.Button("üóëÔ∏è X√≥a h·∫øt Text (Reset)", variant="secondary")
            
            btn_run = gr.Button("üî• PH√ÇN T√çCH", variant="primary")
            
        with gr.Column(scale=1):
            lbl_result = gr.Label(num_top_classes=5, label="Top Prediction")
            debug_text = gr.Textbox(label="Full Input Text", lines=3)

    # --- H√ÄM X·ª¨ L√ù LOGIC UI ---
    
    def toggle_mode(is_image_only):
        """
        H√†m n√†y ch·∫°y khi ng∆∞·ªùi d√πng t√≠ch/b·ªè t√≠ch Checkbox Image Only.
        N√≥ tr·∫£ v·ªÅ tr·∫°ng th√°i m·ªõi cho Accordion v√† c√°c √¥ text.
        """
        if is_image_only:
            # N·∫øu ƒêANG T√çCH (Ch·∫ø ƒë·ªô ·∫£nh):
            # 1. ƒê√≥ng Accordion (open=False)
            # 2. X√≥a s·∫°ch text trong 3 √¥
            # 3. T·∫Øt auto-gen (ƒë·ªÉ logic ph√¢n t√≠ch bi·∫øt l√† kh√¥ng ƒë∆∞·ª£c g·ªçi AI)
            return gr.Accordion(open=False), "", "", "", False
        else:
            # N·∫øu B·ªé T√çCH (Ch·∫ø ƒë·ªô th∆∞·ªùng):
            # 1. M·ªü Accordion (open=True)
            # 2. Gi·ªØ nguy√™n text (ho·∫∑c tr·∫£ v·ªÅ placeholder, ·ªü ƒë√¢y ta ƒë·ªÉ nguy√™n d√πng gr.update())
            # 3. B·∫≠t l·∫°i auto-gen
            # L∆∞u √Ω: gr.update() gi·ªØ nguy√™n gi√° tr·ªã c≈© n·∫øu kh√¥ng truy·ªÅn value
            return gr.Accordion(open=True), gr.update(), gr.update(), gr.update(), True

    # --- S·ª∞ KI·ªÜN ---

    # 1. S·ª± ki·ªán khi b·∫•m Checkbox "Ch·∫ø ƒë·ªô ch·ªâ d√πng ·∫¢nh"
    chk_image_only.change(
        fn=toggle_mode,
        inputs=[chk_image_only],
        outputs=[acc_text_group, txt_slogan, txt_caption, txt_ocr, chk_auto_gen]
    )

    # 2. C√°c n√∫t x√≥a l·∫ª (Gi·ªØ nguy√™n)
    btn_cls_slogan.click(fn=lambda: "", inputs=None, outputs=txt_slogan)
    btn_cls_caption.click(fn=lambda: "", inputs=None, outputs=txt_caption)
    btn_cls_ocr.click(fn=lambda: "", inputs=None, outputs=txt_ocr)

    # 3. N√∫t X√≥a h·∫øt (Reset) -> C≈©ng ph·∫£i ƒë·∫£m b·∫£o Textbox Image Only b·ªã b·ªè t√≠ch
    btn_clear_all.click(
        fn=lambda: ("", "", "", True, False, gr.Accordion(open=True)),
        inputs=None, 
        outputs=[txt_slogan, txt_caption, txt_ocr, chk_auto_gen, chk_image_only, acc_text_group]
    )

    # 4. N√∫t ch·∫°y ch√≠nh
    # C·∫ßn truy·ªÅn chk_image_only v√†o h√†m predict ƒë·ªÉ logic bi·∫øt
    # (L∆∞u √Ω: Logic predict c·∫ßn s·ª≠a nh·∫π ƒë·ªÉ d√πng chk_image_only thay v√¨ chk_auto_gen n·∫øu mu·ªën code s·∫°ch h∆°n,
    # nh∆∞ng ·ªü ƒë√¢y m√¨nh d√πng chk_auto_gen (ƒë∆∞·ª£c sync ng·∫ßm) ƒë·ªÉ t√°i s·ª≠ d·ª•ng code c≈©).
    btn_run.click(
        analyze_and_predict,
        inputs=[input_img, txt_ocr, txt_caption, txt_slogan, model_selector, chk_auto_gen],
        outputs=[debug_text, txt_ocr, txt_caption, txt_slogan, lbl_result]
    )

if __name__ == "__main__":
    demo.launch()