{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2eec739",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e283d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\OnlyPython\\miniconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm # Th∆∞ vi·ªán t·∫°o thanh ti·∫øn tr√¨nh\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "JSON_FILE = \"Super_Final_Dataset_Full.json\" # S·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n c·ªßa b·∫°n\n",
    "IMG_ROOT = \"images\"                         # S·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n c·ªßa b·∫°n\n",
    "BATCH_SIZE = 16    # Gi·∫£m xu·ªëng 8 n·∫øu VRAM < 8GB\n",
    "EPOCHS = 10        # S·ªë v√≤ng l·∫∑p train\n",
    "LEARNING_RATE = 1e-4 # Learning rate cho LoRA (th∆∞·ªùng cao h∆°n fine-tune full model ch√∫t)\n",
    "NUM_CLASSES = 26\n",
    "MAX_CHUNKS = 4\n",
    "\n",
    "# Thi·∫øt l·∫≠p thi·∫øt b·ªã (GPU ∆∞u ti√™n)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- DANH S√ÅCH T√äN CLASS (MAPPING) ---\n",
    "TOPIC_NAMES = {\n",
    "    0: \"Restaurant\", 1: \"Chocolate\", 2: \"Chips/Snacks\", 3: \"Seasoning\",\n",
    "    4: \"Alcohol\", 5: \"Coffee/Tea\", 6: \"Soda/Juice\", 7: \"Cars\", 8: \"Electronics\",\n",
    "    9: \"Phone/TV/Internet\", 10: \"Financial\",\n",
    "    11: \"Other Service\", 12: \"Beauty\", 13: \"Healthcare\", 14: \"Clothing\",\n",
    "    15: \"Games\", 16: \"Home Appliance\", 17: \"Travel\",\n",
    "    18: \"Media\", 19: \"Sports\", 20: \"Shopping\", 21: \"Environment\",\n",
    "    22: \"Animals/Pet Care\", 23: \"Safety\", 24: \"Smoking/Alcohol Abuse\",\n",
    "    25: \"Unclear\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccb139",
   "metadata": {},
   "source": [
    "# DATASET + DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc7b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DATASET CLASS (C·∫≠p nh·∫≠t ƒë·ªÉ h·ªó tr·ª£ Split) ---\n",
    "class AdDataset(Dataset):\n",
    "    def __init__(self, json_data, keys_list, img_root_dir, processor, max_chunks=4):\n",
    "        self.data = json_data\n",
    "        self.keys = keys_list # Ch·ªâ nh·∫≠n danh s√°ch key thu·ªôc t·∫≠p train ho·∫∑c val\n",
    "        self.img_root_dir = img_root_dir\n",
    "        self.processor = processor\n",
    "        self.max_chunks = max_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_key = self.keys[idx]\n",
    "        item = self.data[img_key]\n",
    "        \n",
    "        # X·ª≠ l√Ω ·∫£nh\n",
    "        img_path = os.path.join(self.img_root_dir, img_key)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "\n",
    "        # X·ª≠ l√Ω Text\n",
    "        # Luc dau la: slogan - qa - caption - ocr\n",
    "        text_parts = [\n",
    "            item.get(\"qa\", \"\"),\n",
    "            item.get(\"caption_text\", \"\"),\n",
    "            item.get(\"slogan_text\", \"\"),\n",
    "            item.get(\"ocr_text\", \"\")\n",
    "        ]\n",
    "        full_text = \" \".join([t for t in text_parts if t])\n",
    "\n",
    "        # Tokenize & Chunking\n",
    "        inputs = self.processor(\n",
    "            text=full_text, \n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=77 * self.max_chunks\n",
    "        )\n",
    "        \n",
    "        long_input_ids = inputs['input_ids'][0] \n",
    "        long_attention_mask = inputs['attention_mask'][0]\n",
    "        \n",
    "        chunk_size = 77\n",
    "        input_ids_chunks = []\n",
    "        attention_mask_chunks = []\n",
    "        \n",
    "        for i in range(self.max_chunks):\n",
    "            start = i * chunk_size\n",
    "            end = start + chunk_size\n",
    "            chunk_ids = long_input_ids[start:end]\n",
    "            chunk_mask = long_attention_mask[start:end]\n",
    "            \n",
    "            if len(chunk_ids) < chunk_size:\n",
    "                pad_len = chunk_size - len(chunk_ids)\n",
    "                chunk_ids = torch.cat([chunk_ids, torch.zeros(pad_len, dtype=torch.long)])\n",
    "                chunk_mask = torch.cat([chunk_mask, torch.zeros(pad_len, dtype=torch.long)])\n",
    "            \n",
    "            input_ids_chunks.append(chunk_ids)\n",
    "            attention_mask_chunks.append(chunk_mask)\n",
    "\n",
    "        input_ids = torch.stack(input_ids_chunks)\n",
    "        attention_mask = torch.stack(attention_mask_chunks)\n",
    "        pixel_values = inputs['pixel_values'][0]\n",
    "\n",
    "        label_str = item.get(\"topic_id\")\n",
    "        label = int(label_str) - 1\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5bbb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Data...\n",
      "Scanning labels for split stratification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59282/59282 [00:00<00:00, 1516212.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split Summary:\n",
      "   - Train set: 47425 samples\n",
      "   - Val set:   5928 samples\n",
      "   - Test set:  5929 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load d·ªØ li·ªáu & Split\n",
    "print(\"Loading Data...\")\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "all_keys = list(full_data.keys())\n",
    "\n",
    "# L·∫•y labels ra ƒë·ªÉ stratify split (ƒë·∫£m b·∫£o chia ƒë·ªÅu c√°c class)\n",
    "all_labels_for_split = []\n",
    "valid_keys = []\n",
    "\n",
    "print(\"Scanning labels for split stratification...\")\n",
    "for k in tqdm(all_keys):\n",
    "    try:\n",
    "        lbl = int(full_data[k]['topic_id']) - 1\n",
    "        if 0 <= lbl < NUM_CLASSES:\n",
    "            all_labels_for_split.append(lbl)\n",
    "            valid_keys.append(k)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# --- CHIA 3 T·∫¨P: TRAIN (80%) - VAL (10%) - TEST (10%) ---\n",
    "# B∆∞·ªõc 1: Chia Train (80%) v√† Temp (20%)\n",
    "train_keys, temp_keys, train_labels, temp_labels = train_test_split(\n",
    "    valid_keys, all_labels_for_split, test_size=0.2, random_state=42, stratify=all_labels_for_split\n",
    ")\n",
    "\n",
    "# B∆∞·ªõc 2: Chia Temp th√†nh Val (50% c·ªßa Temp = 10% t·ªïng) v√† Test (50% c·ªßa Temp = 10% t·ªïng)\n",
    "val_keys, test_keys = train_test_split(\n",
    "    temp_keys, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"Dataset Split Summary:\")\n",
    "print(f\"   - Train set: {len(train_keys)} samples\")\n",
    "print(f\"   - Val set:   {len(val_keys)} samples\")\n",
    "print(f\"   - Test set:  {len(test_keys)} samples\")\n",
    "\n",
    "train_dataset = AdDataset(full_data, train_keys, IMG_ROOT, processor, MAX_CHUNKS)\n",
    "val_dataset = AdDataset(full_data, val_keys, IMG_ROOT, processor, MAX_CHUNKS)\n",
    "test_dataset = AdDataset(full_data, test_keys, IMG_ROOT, processor, MAX_CHUNKS) # Dataset m·ªõi cho Test\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # Loader m·ªõi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d76dc40",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234f3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. MODEL CLASS (Gi·ªØ nguy√™n c·∫•u h√¨nh LoRA c·ªßa b·∫°n) ---\n",
    "class MultimodalCLIPClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, base_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(base_model_name, use_safetensors=True)\n",
    "        \n",
    "        # C·∫•u h√¨nh LoRA nh∆∞ b·∫°n y√™u c·∫ßu (gi·ªØ nguy√™n params)\n",
    "        config = LoraConfig(\n",
    "            r=8, \n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"], \n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        self.clip = get_peft_model(self.clip, config)\n",
    "        self.classifier = nn.Linear(self.clip.config.projection_dim * 2, num_classes)\n",
    "        \n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        batch_size, num_chunks, seq_len = input_ids.shape\n",
    "        \n",
    "        vision_outputs = self.clip.base_model.model.vision_model(pixel_values=pixel_values)\n",
    "        image_embeds = self.clip.base_model.model.visual_projection(vision_outputs[1])\n",
    "\n",
    "        flat_input_ids = input_ids.view(-1, seq_len) \n",
    "        flat_attention_mask = attention_mask.view(-1, seq_len)\n",
    "        \n",
    "        text_outputs = self.clip.base_model.model.text_model(\n",
    "            input_ids=flat_input_ids, \n",
    "            attention_mask=flat_attention_mask\n",
    "        )\n",
    "        text_embeds_flat = self.clip.base_model.model.text_projection(text_outputs[1])\n",
    "        text_embeds = text_embeds_flat.view(batch_size, num_chunks, -1)\n",
    "        text_embeds = torch.mean(text_embeds, dim=1) \n",
    "\n",
    "        combined_features = torch.cat((image_embeds, text_embeds), dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0adecc1",
   "metadata": {},
   "source": [
    "# TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69a0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAINING ENGINE ---\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        loss, logits = model(pixel_values, input_ids, attention_mask, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # K·ªπ thu·∫≠t: Gradient Clipping ƒë·ªÉ tr√°nh gradient n·ªï (g√¢y l·ªói NaN)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        loop.set_description(f\"Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            loss, logits = model(pixel_values, input_ids, attention_mask, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b082b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model...\n",
      "trainable params: 491,520 || all params: 151,768,833 || trainable%: 0.3239\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.3928: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [35:41<00:00,  1.38it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [03:15<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result - Train Loss: 1.2338 | Train Acc: 68.60%\n",
      "Result - Val Loss: 0.7585   | Val Acc: 80.28%\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5145: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [23:39<00:00,  2.09it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [01:49<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result - Train Loss: 0.6207 | Train Acc: 83.41%\n",
      "Result - Val Loss: 0.6044   | Val Acc: 83.11%\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1314: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [23:41<00:00,  2.09it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [01:49<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result - Train Loss: 0.4630 | Train Acc: 87.60%\n",
      "Result - Val Loss: 0.5688   | Val Acc: 83.50%\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0010: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [23:43<00:00,  2.08it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [01:50<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result - Train Loss: 0.3554 | Train Acc: 90.28%\n",
      "Result - Val Loss: 0.5608   | Val Acc: 83.94%\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0014: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [23:42<00:00,  2.08it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [01:50<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result - Train Loss: 0.2709 | Train Acc: 92.71%\n",
      "Result - Val Loss: 0.5853   | Val Acc: 83.96%\n",
      "‚è∏ No improvement (1/2)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1124: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [23:43<00:00,  2.08it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [01:50<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result - Train Loss: 0.2044 | Train Acc: 94.61%\n",
      "Result - Val Loss: 0.6127   | Val Acc: 83.79%\n",
      "‚è∏ No improvement (2/2)\n",
      "üõë Early stopping triggered!\n",
      "Training Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Kh·ªüi t·∫°o Model\n",
    "print(\"Initializing Model...\")\n",
    "model = MultimodalCLIPClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "model.clip.print_trainable_parameters()\n",
    "\n",
    "# Optimizer (Ch·ªâ t·ªëi ∆∞u tham s·ªë c√≥ requires_grad=True)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "# Loop Train\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 2\n",
    "early_stop_counter = 0\n",
    "\n",
    "print(\"\\n--- START TRAINING ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Result - Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Result - Val Loss: {val_loss:.4f}   | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # L∆∞u Best Model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"clip_qa_cap_slo_ocr.pth\")\n",
    "        print(\"--> Saved Best Model!\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"‚è∏ No improvement ({early_stop_counter}/{patience})\")\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"üõë Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(\"Training Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e50471",
   "metadata": {},
   "source": [
    "# EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66d7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. H√ÄM TEST TR√äN T·∫¨P TEST (M·ªõi th√™m) ---\n",
    "def test_final_model(model, test_loader, device):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ƒêANG CH·∫†Y ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST (FINAL EVALUATION)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            _, logits = model(pixel_values, input_ids, attention_mask, labels)\n",
    "            \n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # T√≠nh to√°n c√°c ch·ªâ s·ªë\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\n‚úÖ TEST ACCURACY: {acc*100:.2f}%\")\n",
    "    \n",
    "    # T·∫°o danh s√°ch t√™n class cho b√°o c√°o ƒë·∫πp\n",
    "    target_names = [TOPIC_NAMES[i] for i in range(NUM_CLASSES)]\n",
    "    \n",
    "    print(\"\\nüìä CHI TI·∫æT THEO T·ª™NG CLASS:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "    \n",
    "    print(\"\\n(L∆∞u √Ω: B·∫°n c√≥ th·ªÉ d√πng Confusion Matrix ƒë·ªÉ xem chi ti·∫øt nh·∫ßm l·∫´n gi·ªØa c√°c l·ªõp n·∫øu c·∫ßn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef811a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model for Final Testing...\n",
      "\n",
      "==================================================\n",
      "ƒêANG CH·∫†Y ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST (FINAL EVALUATION)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [03:18<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ TEST ACCURACY: 84.36%\n",
      "\n",
      "üìä CHI TI·∫æT THEO T·ª™NG CLASS:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           Restaurant     0.8988    0.8988    0.8988       405\n",
      "            Chocolate     0.8469    0.8949    0.8702       371\n",
      "         Chips/Snacks     0.7380    0.7753    0.7562       178\n",
      "            Seasoning     0.9200    0.6389    0.7541        72\n",
      "              Alcohol     0.9355    0.9321    0.9338       280\n",
      "           Coffee/Tea     0.7794    0.7571    0.7681        70\n",
      "           Soda/Juice     0.8828    0.9435    0.9121       407\n",
      "                 Cars     0.9563    0.9617    0.9590       705\n",
      "          Electronics     0.8266    0.8916    0.8579       369\n",
      "    Phone/TV/Internet     0.8088    0.7534    0.7801        73\n",
      "            Financial     0.7885    0.8542    0.8200       144\n",
      "        Other Service     0.5733    0.3839    0.4599       112\n",
      "               Beauty     0.8997    0.9447    0.9217       579\n",
      "           Healthcare     0.7556    0.7010    0.7273        97\n",
      "             Clothing     0.8635    0.9301    0.8956       830\n",
      "                Games     0.7609    0.6250    0.6863        56\n",
      "       Home Appliance     0.6712    0.6364    0.6533        77\n",
      "               Travel     0.8382    0.8593    0.8486       199\n",
      "                Media     0.5600    0.5785    0.5691       121\n",
      "               Sports     0.6996    0.6680    0.6834       244\n",
      "             Shopping     0.8254    0.5843    0.6842       178\n",
      "          Environment     0.6579    0.5952    0.6250        42\n",
      "     Animals/Pet Care     0.8243    0.6854    0.7485        89\n",
      "               Safety     0.6542    0.7000    0.6763       100\n",
      "Smoking/Alcohol Abuse     0.9123    0.8387    0.8739        62\n",
      "              Unclear     0.2812    0.1304    0.1782        69\n",
      "\n",
      "             accuracy                         0.8436      5929\n",
      "            macro avg     0.7753    0.7370    0.7516      5929\n",
      "         weighted avg     0.8378    0.8436    0.8384      5929\n",
      "\n",
      "\n",
      "(L∆∞u √Ω: B·∫°n c√≥ th·ªÉ d√πng Confusion Matrix ƒë·ªÉ xem chi ti·∫øt nh·∫ßm l·∫´n gi·ªØa c√°c l·ªõp n·∫øu c·∫ßn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- INFERENCE ON TEST SET ---\n",
    "# Sau khi train xong, load l·∫°i model t·ªët nh·∫•t ƒë·ªÉ test\n",
    "print(\"Loading Best Model for Final Testing...\")\n",
    "\n",
    "# Reset model v·ªÅ tr·∫°ng th√°i kh·ªüi t·∫°o r·ªìi load weight\n",
    "final_model = MultimodalCLIPClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "final_model.load_state_dict(torch.load(\"clip_qa_cap_slo_ocr.pth\", map_location=device, weights_only=True))\n",
    "\n",
    "# G·ªçi h√†m test\n",
    "test_final_model(final_model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
