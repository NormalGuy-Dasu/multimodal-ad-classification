{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9929a4e6",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34f712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\OnlyPython\\miniconda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR_HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d3a574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForImagesAndTextClassification, ViltConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# D√πng model ViLT ƒë√£ pretrain s·∫µn cho t√°c v·ª• MLM (Masked Language Modeling)\n",
    "# Model n√†y r·∫•t nh·∫π v√† hi·ªáu qu·∫£ cho b√†i to√°n Visual Question Answering ho·∫∑c Classification\n",
    "MODEL_NAME = \"dandelin/vilt-b32-mlm\" \n",
    "\n",
    "JSON_FILE = \"Super_Final_Dataset_Full.json\"  # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng\n",
    "IMG_ROOT = \"images\"                          # ƒê·∫£m b·∫£o ƒë∆∞·ªùng d·∫´n ƒë√∫ng\n",
    "BATCH_SIZE = 16    \n",
    "EPOCHS = 10         \n",
    "LEARNING_RATE = 5e-5 \n",
    "MAX_LEN = 40       # ƒê·ªô d√†i text t·ªëi ƒëa\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- DANH S√ÅCH T√äN CLASS (MAPPING) ---\n",
    "TOPIC_NAMES = {\n",
    "    0: \"Restaurant\", 1: \"Chocolate\", 2: \"Chips/Snacks\", 3: \"Seasoning\",\n",
    "    4: \"Alcohol\", 5: \"Coffee/Tea\", 6: \"Soda/Juice\", 7: \"Cars\", 8: \"Electronics\",\n",
    "    9: \"Phone/TV/Internet\", 10: \"Financial\",\n",
    "    11: \"Other Service\", 12: \"Beauty\", 13: \"Healthcare\", 14: \"Clothing\",\n",
    "    15: \"Games\", 16: \"Home Appliance\", 17: \"Travel\",\n",
    "    18: \"Media\", 19: \"Sports\", 20: \"Shopping\", 21: \"Environment\",\n",
    "    22: \"Animals/Pet Care\", 23: \"Safety\", 24: \"Smoking/Alcohol Abuse\",\n",
    "    25: \"Unclear\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0859a3a",
   "metadata": {},
   "source": [
    "# DATASET + DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa82ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASS DATASET (ƒê√£ ch·ªânh s·ª≠a cho ViLT) ---\n",
    "class AdDataset(Dataset):\n",
    "    def __init__(self, full_data_dict, keys_list, img_root, processor, label2id, max_len=128):\n",
    "        self.full_data = full_data_dict\n",
    "        self.keys = keys_list # Danh s√°ch c√°c key (vd: \"2/71762.jpg\") thu·ªôc t·∫≠p n√†y\n",
    "        self.img_root = img_root\n",
    "        self.processor = processor\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        item = self.full_data[key]\n",
    "        \n",
    "        # 1. Load ·∫£nh\n",
    "        img_path = os.path.join(self.img_root, key)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            # --- [FIX QUAN TR·ªåNG] ---\n",
    "            # √âp ·∫£nh v·ªÅ k√≠ch th∆∞·ªõc vu√¥ng 384x384 ƒë·ªÉ tr√°nh l·ªói l·ªách size trong Batch\n",
    "            image = image.resize((384, 384)) \n",
    "            # ------------------------\n",
    "\n",
    "        except:\n",
    "            # ·∫¢nh l·ªói -> T·∫°o ·∫£nh ƒëen\n",
    "            image = Image.new('RGB', (384, 384), color='black')\n",
    "\n",
    "        # 2. X·ª≠ l√Ω Text (N·ªëi chu·ªói thay v√¨ Chunking)\n",
    "        # ƒê√£ th√™m field \"qa\" v√†o nh∆∞ b·∫°n y√™u c·∫ßu\n",
    "        # Luc dau la: qa - slogan - caption - ocr\n",
    "        text = (\n",
    "            f\"QA: {item.get('qa', '')} \"\n",
    "            f\"Caption: {item.get('caption_text', '')} \"\n",
    "            f\"Slogan: {item.get('slogan_text', '')} \"\n",
    "            f\"OCR: {item.get('ocr_text', '')}\"\n",
    "        )\n",
    "\n",
    "        # 3. Label\n",
    "        label_str = str(item['topic_id'])\n",
    "        label = self.label2id[label_str]\n",
    "\n",
    "        # 4. Processor (ViLT x·ª≠ l√Ω c·∫£ ·∫£nh v√† text c√πng l√∫c)\n",
    "        encoding = self.processor(\n",
    "            image, \n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=self.max_len\n",
    "        )\n",
    "\n",
    "        # Squeeze ƒë·ªÉ b·ªè batch dimension th·ª´a\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'],\n",
    "            'token_type_ids': encoding['token_type_ids'],\n",
    "            'attention_mask': encoding['attention_mask'],\n",
    "            'pixel_values': encoding['pixel_values'],\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ddd551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data & Processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\OnlyPython\\miniconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in F:\\OnlyPython\\cache\\huggingface\\hub\\models--dandelin--vilt-b32-mlm. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 classes.\n",
      "Scanning data for split stratification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59282/59282 [00:00<00:00, 73823.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split Summary:\n",
      "   - Train set: 47425 samples\n",
      "   - Val set:   5928 samples\n",
      "   - Test set:  5929 samples\n",
      "Data Preparation Complete!\n"
     ]
    }
   ],
   "source": [
    "# --- PH·∫¶N CHU·∫®N B·ªä D·ªÆ LI·ªÜU ---\n",
    "\n",
    "print(\"Loading Data & Processor...\")\n",
    "# D√πng ViltProcessor thay v√¨ CLIPProcessor\n",
    "processor = ViltProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "with open(JSON_FILE, 'r', encoding='utf-8') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# 1. T·∫°o Mapping Label (Quan tr·ªçng: Map ID sang 0,1,2...)\n",
    "# Qu√©t to√†n b·ªô file ƒë·ªÉ l·∫•y t·∫•t c·∫£ topic_id duy nh·∫•t\n",
    "all_unique_labels = set()\n",
    "for k, v in full_data.items():\n",
    "    all_unique_labels.add(str(v['topic_id']))\n",
    "\n",
    "# Sort ƒë·ªÉ th·ª© t·ª± lu√¥n c·ªë ƒë·ªãnh: '1', '2', ..., '25'\n",
    "sorted_labels = sorted(list(all_unique_labels), key=lambda x: int(x) if x.isdigit() else x)\n",
    "label2id = {label: i for i, label in enumerate(sorted_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "NUM_CLASSES = len(label2id)\n",
    "print(f\"Found {NUM_CLASSES} classes.\")\n",
    "\n",
    "# 2. L·ªçc keys h·ª£p l·ªá ƒë·ªÉ Split\n",
    "all_keys_valid = []\n",
    "all_labels_for_split = []\n",
    "\n",
    "print(\"Scanning data for split stratification...\")\n",
    "for k in tqdm(list(full_data.keys())):\n",
    "    # Ki·ªÉm tra xem ·∫£nh c√≥ t·ªìn t·∫°i kh√¥ng (Optional nh∆∞ng n√™n l√†m)\n",
    "    if not os.path.exists(os.path.join(IMG_ROOT, k)):\n",
    "        continue\n",
    "        \n",
    "    topic_id = str(full_data[k]['topic_id'])\n",
    "    if topic_id in label2id:\n",
    "        all_keys_valid.append(k)\n",
    "        all_labels_for_split.append(label2id[topic_id]) # D√πng index (0-24) ƒë·ªÉ stratify\n",
    "\n",
    "# --- CHIA 3 T·∫¨P: TRAIN (80%) - VAL (10%) - TEST (10%) ---\n",
    "# B∆∞·ªõc 1: Chia Train (80%) v√† Temp (20%)\n",
    "train_keys, temp_keys, train_labels, temp_labels = train_test_split(\n",
    "    all_keys_valid, all_labels_for_split, test_size=0.2, random_state=42, stratify=all_labels_for_split\n",
    ")\n",
    "\n",
    "# B∆∞·ªõc 2: Chia Temp th√†nh Val (50% c·ªßa Temp = 10% t·ªïng) v√† Test (50% c·ªßa Temp = 10% t·ªïng)\n",
    "val_keys, test_keys = train_test_split(\n",
    "    temp_keys, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\"Dataset Split Summary:\")\n",
    "print(f\"   - Train set: {len(train_keys)} samples\")\n",
    "print(f\"   - Val set:   {len(val_keys)} samples\")\n",
    "print(f\"   - Test set:  {len(test_keys)} samples\")\n",
    "\n",
    "# T·∫°o Dataset Instance (Truy·ªÅn full_data + keys c·ªßa t·ª´ng t·∫≠p)\n",
    "train_dataset = AdDataset(full_data, train_keys, IMG_ROOT, processor, label2id, MAX_LEN)\n",
    "val_dataset = AdDataset(full_data, val_keys, IMG_ROOT, processor, label2id, MAX_LEN)\n",
    "test_dataset = AdDataset(full_data, test_keys, IMG_ROOT, processor, label2id, MAX_LEN)\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Data Preparation Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980834cf",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79971929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vilt_lora_model(num_labels, id2label, label2id):\n",
    "    print(f\"Loading ViLT model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Load model g·ªëc d√†nh cho classification\n",
    "    model = ViltForImagesAndTextClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        num_images=1,                  # <--- TH√äM D√íNG N√ÄY ƒê·ªÇ S·ª¨A L·ªñI\n",
    "        ignore_mismatched_sizes=True,   # B·∫Øt bu·ªôc v√¨ ta thay ƒë·ªïi s·ªë l∆∞·ª£ng class output\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    \n",
    "    # C·∫•u h√¨nh LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,           # Rank\n",
    "        lora_alpha=16,  # Alpha\n",
    "        target_modules=[\"query\", \"value\"], # C√°c module trong Self-Attention\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        modules_to_save=[\"classifier\"] # Quan tr·ªçng: Train l·∫°i c·∫£ l·ªõp classifier cu·ªëi c√πng\n",
    "    )\n",
    "    \n",
    "    # √Åp d·ª•ng LoRA\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # In ra s·ªë l∆∞·ª£ng tham s·ªë trainable\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8f49f",
   "metadata": {},
   "source": [
    "# TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e42a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # ƒê·∫©y d·ªØ li·ªáu l√™n GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # ViLT t·ª± t√≠nh loss n·∫øu truy·ªÅn labels v√†o, nh∆∞ng ta c√≥ th·ªÉ l·∫•y logits ƒë·ªÉ t·ª± t√≠nh\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # T√≠nh accuracy training s∆° b·ªô\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = correct / total\n",
    "    \n",
    "    return avg_loss, acc, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6024cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViLT model: dandelin/vilt-b32-mlm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of ViltForImagesAndTextClassification were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 907,034 || all params: 113,114,164 || trainable%: 0.8019\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:47<00:00,  1.99it/s, loss=0.116]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0323 | Train Acc: 0.7222\n",
      "Val Loss:   0.7647 | Val Acc:   0.7913\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:43<00:00,  2.00it/s, loss=0.24]  \n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6929 | Train Acc: 0.8075\n",
      "Val Loss:   0.6877 | Val Acc:   0.8109\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:43<00:00,  2.00it/s, loss=0.0474]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6084 | Train Acc: 0.8306\n",
      "Val Loss:   0.6548 | Val Acc:   0.8209\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:42<00:00,  2.00it/s, loss=2.16]  \n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5503 | Train Acc: 0.8451\n",
      "Val Loss:   0.6414 | Val Acc:   0.8239\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:44<00:00,  2.00it/s, loss=0.00288]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:12<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4992 | Train Acc: 0.8575\n",
      "Val Loss:   0.6246 | Val Acc:   0.8295\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:43<00:00,  2.00it/s, loss=0.0429]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4547 | Train Acc: 0.8700\n",
      "Val Loss:   0.6295 | Val Acc:   0.8268\n",
      "‚è∏ No improvement (1/2)\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:44<00:00,  2.00it/s, loss=0.00842]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4151 | Train Acc: 0.8796\n",
      "Val Loss:   0.6227 | Val Acc:   0.8283\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:44<00:00,  2.00it/s, loss=0.0405]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3751 | Train Acc: 0.8909\n",
      "Val Loss:   0.6173 | Val Acc:   0.8316\n",
      "--> Saved Best Model!\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:44<00:00,  2.00it/s, loss=0.0451]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3384 | Train Acc: 0.9021\n",
      "Val Loss:   0.6202 | Val Acc:   0.8338\n",
      "‚è∏ No improvement (1/2)\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2965/2965 [24:44<00:00,  2.00it/s, loss=0.000579]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:13<00:00,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3026 | Train Acc: 0.9126\n",
      "Val Loss:   0.6308 | Val Acc:   0.8316\n",
      "‚è∏ No improvement (2/2)\n",
      "üõë Early stopping triggered!\n",
      "Training Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Init Model (ViLT + LoRA)\n",
    "model = get_vilt_lora_model(len(label2id), id2label, label2id)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# 5. Optimizer & Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. Training Loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "early_stop_counter = 0\n",
    "save_path = \"vilt_qa_cap_slo_ocr.pth\"\n",
    "\n",
    "print(\"\\n--- START TRAINING ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Save Best Model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"--> Saved Best Model!\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"‚è∏ No improvement ({early_stop_counter}/{patience})\")\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"üõë Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6162cc",
   "metadata": {},
   "source": [
    "# EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445e1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. H√ÄM TEST TR√äN T·∫¨P TEST (ƒê√£ s·ª≠a cho ViLT) ---\n",
    "def test_final_model(model, test_loader, device, label2id):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ƒêANG CH·∫†Y ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST (FINAL EVALUATION)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # ƒê·∫©y d·ªØ li·ªáu v√†o device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass (ViLT tr·∫£ v·ªÅ object, l·∫•y .logits)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # L·∫•y class c√≥ x√°c su·∫•t cao nh·∫•t\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # --- T√çNH TO√ÅN CH·ªà S·ªê ---\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\n‚úÖ TEST ACCURACY: {acc*100:.2f}%\")\n",
    "    \n",
    "    # T·∫°o danh s√°ch t√™n class theo ƒë√∫ng th·ª© t·ª± index 0, 1, 2... c·ªßa model\n",
    "    # V√¨ label2id c√≥ th·ªÉ kh√¥ng ƒë·ªß 26 class (n·∫øu d·ªØ li·ªáu thi·∫øu), ta c·∫ßn map ƒë·ªông\n",
    "    # Logic: Model Class ID -> Topic ID G·ªëc -> T√™n Ti·∫øng Anh\n",
    "    \n",
    "    # ƒê·∫£o ng∆∞·ª£c label2id ƒë·ªÉ l·∫•y topic_id g·ªëc t·ª´ model output (0 -> \"1\", 1 -> \"2\"...)\n",
    "    id2label = {v: k for k, v in label2id.items()} \n",
    "    \n",
    "    target_names = []\n",
    "    unique_classes_in_model = sorted(label2id.values()) # [0, 1, 2, ..., N]\n",
    "    \n",
    "    for class_idx in unique_classes_in_model:\n",
    "        original_topic_id_str = id2label[class_idx] # V√≠ d·ª•: \"1\", \"25\"\n",
    "        \n",
    "        # Topic ID trong file json l√† 1-based (1..25), nh∆∞ng ta c·∫ßn map sang key c·ªßa TOPIC_NAMES\n",
    "        # Gi·∫£ s·ª≠ topic \"1\" t∆∞∆°ng ·ª©ng v·ªõi key 0 trong TOPIC_NAMES (Restaurant)\n",
    "        # N·∫øu logic c·ªßa b·∫°n l√† Topic 1 = Key 0, Topic 2 = Key 1:\n",
    "        try:\n",
    "            topic_key = int(original_topic_id_str) - 1 \n",
    "            name = TOPIC_NAMES.get(topic_key, f\"Topic {original_topic_id_str}\")\n",
    "        except:\n",
    "            name = f\"Topic {original_topic_id_str}\"\n",
    "            \n",
    "        target_names.append(name)\n",
    "    \n",
    "    print(\"\\nüìä CHI TI·∫æT THEO T·ª™NG CLASS:\")\n",
    "    # digits=4 ƒë·ªÉ hi·ªÉn th·ªã 4 s·ªë sau d·∫•u ph·∫©y cho ch√≠nh x√°c\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "    \n",
    "    print(\"\\n(L∆∞u √Ω: B·∫°n c√≥ th·ªÉ d√πng Confusion Matrix ƒë·ªÉ xem chi ti·∫øt nh·∫ßm l·∫´n gi·ªØa c√°c l·ªõp n·∫øu c·∫ßn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d5767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Best Model from best_vilt_lora_model_loss.pth for Final Testing...\n",
      "Loading ViLT model: dandelin/vilt-b32-mlm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForImagesAndTextClassification were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 907,034 || all params: 113,114,164 || trainable%: 0.8019\n",
      "Model weights loaded successfully!\n",
      "\n",
      "==================================================\n",
      "ƒêANG CH·∫†Y ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST (FINAL EVALUATION)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 371/371 [02:12<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ TEST ACCURACY: 83.22%\n",
      "\n",
      "üìä CHI TI·∫æT THEO T·ª™NG CLASS:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           Restaurant     0.8819    0.8667    0.8742       405\n",
      "            Chocolate     0.8308    0.9003    0.8642       371\n",
      "         Chips/Snacks     0.7486    0.7528    0.7507       178\n",
      "            Seasoning     0.8889    0.7778    0.8296        72\n",
      "              Alcohol     0.9223    0.9321    0.9272       280\n",
      "           Coffee/Tea     0.8714    0.8714    0.8714        70\n",
      "           Soda/Juice     0.8709    0.9115    0.8908       407\n",
      "                 Cars     0.9627    0.9518    0.9572       705\n",
      "          Electronics     0.8443    0.8672    0.8556       369\n",
      "    Phone/TV/Internet     0.7015    0.6438    0.6714        73\n",
      "            Financial     0.7500    0.8333    0.7895       144\n",
      "        Other Service     0.3355    0.4643    0.3895       112\n",
      "               Beauty     0.9083    0.9067    0.9075       579\n",
      "           Healthcare     0.7246    0.5155    0.6024        97\n",
      "             Clothing     0.8726    0.8747    0.8736       830\n",
      "                Games     0.6379    0.6607    0.6491        56\n",
      "       Home Appliance     0.7792    0.7792    0.7792        77\n",
      "               Travel     0.8646    0.8342    0.8491       199\n",
      "                Media     0.6339    0.5868    0.6094       121\n",
      "               Sports     0.7009    0.6721    0.6862       244\n",
      "             Shopping     0.8252    0.6629    0.7352       178\n",
      "          Environment     0.5556    0.7143    0.6250        42\n",
      "     Animals/Pet Care     0.6961    0.7978    0.7435        89\n",
      "               Safety     0.7579    0.7200    0.7385       100\n",
      "Smoking/Alcohol Abuse     0.9310    0.8710    0.9000        62\n",
      "              Unclear     0.2609    0.1739    0.2087        69\n",
      "\n",
      "             accuracy                         0.8322      5929\n",
      "            macro avg     0.7599    0.7516    0.7530      5929\n",
      "         weighted avg     0.8337    0.8322    0.8317      5929\n",
      "\n",
      "\n",
      "(L∆∞u √Ω: B·∫°n c√≥ th·ªÉ d√πng Confusion Matrix ƒë·ªÉ xem chi ti·∫øt nh·∫ßm l·∫´n gi·ªØa c√°c l·ªõp n·∫øu c·∫ßn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. INFERENCE ON TEST SET ---\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n file model ƒë√£ l∆∞u (ƒë·∫£m b·∫£o t√™n file kh·ªõp v·ªõi l√∫c train)\n",
    "SAVED_MODEL_PATH = \"vilt_qa_cap_slo_ocr.pth\" \n",
    "\n",
    "print(f\"Loading Best Model from {SAVED_MODEL_PATH} for Final Testing...\")\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o l·∫°i ki·∫øn tr√∫c model (ph·∫£i gi·ªëng h·ªát l√∫c train)\n",
    "# L∆∞u √Ω: H√†m get_vilt_lora_model c·∫ßn ƒë∆∞·ª£c define ·ªü c√°c cell tr∆∞·ªõc ƒë√≥\n",
    "# label2id v√† id2label l·∫•y t·ª´ b∆∞·ªõc prepare_data\n",
    "final_model = get_vilt_lora_model(len(label2id), id2label, label2id)\n",
    "final_model.to(DEVICE)\n",
    "\n",
    "# 2. Load tr·ªçng s·ªë (weights)\n",
    "try:\n",
    "    final_model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=DEVICE, weights_only=True))\n",
    "    print(\"Model weights loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\")\n",
    "    print(\"H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file .pth\")\n",
    "\n",
    "# 3. G·ªçi h√†m test\n",
    "test_final_model(final_model, test_loader, DEVICE, label2id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
